#!/usr/bin/env python3
"""
ULTRA FAST Mark price calculation script - WITH DIAGNOSTICS (FIXED v2)
Incluye an√°lisis de gaps y per√≠odos vac√≠os - corregido manejo de par√°metros SQL
"""

import sys
import os
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from sqlalchemy import text
from pathlib import Path
from typing import Dict, List, Optional, Tuple

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from src.database.connection import db_manager
from src.utils.logger import get_validation_logger
from config.settings import settings

log = get_validation_logger()

def diagnose_data_gaps(symbol: str) -> Dict:
    """Diagnosticar gaps en los datos de orderbook y mark prices"""
    log.info(f"üîç Diagnosticando gaps de datos para {symbol}...")
    
    with db_manager.get_session() as session:
        # 1. Obtener rango de fechas de orderbook v√°lido
        orderbook_range_query = text("""
            SELECT 
                MIN(timestamp) as min_date,
                MAX(timestamp) as max_date,
                COUNT(DISTINCT DATE(timestamp)) as days_with_data,
                COUNT(*) as total_records,
                COUNT(CASE WHEN valid_for_trading = TRUE THEN 1 END) as valid_records
            FROM orderbook 
            WHERE symbol = :symbol
            AND bid1_price IS NOT NULL 
            AND ask1_price IS NOT NULL
        """)
        
        ob_range = session.execute(orderbook_range_query, {'symbol': symbol}).fetchone()
        
        if not ob_range.min_date or not ob_range.max_date:
            log.warning(f"No orderbook data found for {symbol}")
            return {'error': 'No orderbook data'}
        
        # 2. Analizar gaps por d√≠a - VERSI√ìN CORREGIDA
        # Primero, obtener todos los d√≠as con datos
        daily_counts_query = """
            SELECT 
                DATE(timestamp) as trading_date,
                COUNT(*) as records_count,
                MIN(timestamp) as first_record,
                MAX(timestamp) as last_record,
                COUNT(CASE WHEN valid_for_trading = TRUE THEN 1 END) as valid_count
            FROM orderbook 
            WHERE symbol = %(symbol)s
            AND bid1_price IS NOT NULL 
            AND ask1_price IS NOT NULL
            GROUP BY DATE(timestamp)
            ORDER BY trading_date
        """
        
        daily_counts = pd.read_sql(
            daily_counts_query,
            session.bind,
            params={'symbol': symbol}
        )
        
        # Generar serie completa de fechas
        date_range = pd.date_range(
            start=ob_range.min_date.date(),
            end=ob_range.max_date.date(),
            freq='D'
        )
        
        # Crear DataFrame con todas las fechas
        all_dates_df = pd.DataFrame({
            'date': date_range.date
        })
        
        # Merge con los datos reales
        daily_data = all_dates_df.merge(
            daily_counts.rename(columns={'trading_date': 'date'}),
            on='date',
            how='left'
        )
        
        # Rellenar valores faltantes
        daily_data['records'] = daily_data['records_count'].fillna(0).astype(int)
        daily_data['valid_records'] = daily_data['valid_count'].fillna(0).astype(int)
        
        # Clasificar calidad de datos
        daily_data['data_quality'] = daily_data['records'].apply(
            lambda x: 'NO_DATA' if x == 0 
            else 'SPARSE' if x < 100 
            else 'PARTIAL' if x < 500 
            else 'COMPLETE'
        )
        
        # 3. Identificar per√≠odos problem√°ticos
        no_data_days = daily_data[daily_data['data_quality'] == 'NO_DATA']
        sparse_days = daily_data[daily_data['data_quality'] == 'SPARSE']
        
        # 4. Analizar gaps consecutivos
        gaps = []
        current_gap = None
        
        for idx, row in daily_data.iterrows():
            if row['data_quality'] in ['NO_DATA', 'SPARSE']:
                if current_gap is None:
                    current_gap = {
                        'start': row['date'],
                        'end': row['date'],
                        'days': 1,
                        'type': row['data_quality']
                    }
                else:
                    current_gap['end'] = row['date']
                    current_gap['days'] += 1
            else:
                if current_gap is not None:
                    gaps.append(current_gap)
                    current_gap = None
        
        if current_gap is not None:
            gaps.append(current_gap)
        
        # 5. Verificar mark prices existentes
        markprice_check_query = text("""
            SELECT 
                MIN(timestamp) as min_date,
                MAX(timestamp) as max_date,
                COUNT(*) as total_records,
                COUNT(DISTINCT DATE(timestamp)) as days_with_markprices
            FROM mark_prices 
            WHERE symbol = :symbol
        """)
        
        mp_check = session.execute(markprice_check_query, {'symbol': symbol}).fetchone()
        
        # 6. An√°lisis espec√≠fico de junio
        june_analysis_query = """
            SELECT 
                DATE(timestamp) as date,
                COUNT(*) as orderbook_records,
                COUNT(CASE WHEN valid_for_trading = TRUE THEN 1 END) as valid_records
            FROM orderbook 
            WHERE symbol = %(symbol)s
            AND timestamp >= '2024-06-01' 
            AND timestamp < '2024-07-01'
            GROUP BY DATE(timestamp)
            ORDER BY date
        """
        
        june_data = pd.read_sql(
            june_analysis_query,
            session.bind,
            params={'symbol': symbol}
        )
        
        # 7. An√°lisis detallado de d√≠as problem√°ticos
        problem_days_query = """
            SELECT 
                DATE(timestamp) as date,
                COUNT(*) as total_records,
                COUNT(CASE WHEN valid_for_trading = TRUE THEN 1 END) as valid_records,
                MIN(timestamp::time) as first_time,
                MAX(timestamp::time) as last_time,
                COUNT(DISTINCT EXTRACT(HOUR FROM timestamp)) as hours_with_data
            FROM orderbook 
            WHERE symbol = %(symbol)s
            AND bid1_price IS NOT NULL 
            AND ask1_price IS NOT NULL
            GROUP BY DATE(timestamp)
            HAVING COUNT(*) < 500
            ORDER BY date
        """
        
        problem_days = pd.read_sql(
            problem_days_query,
            session.bind,
            params={'symbol': symbol}
        )
        
        # 8. An√°lisis de gaps largos (m√°s de 3 d√≠as consecutivos)
        long_gaps = [gap for gap in gaps if gap['days'] >= 3]
        
        return {
            'symbol': symbol,
            'orderbook_range': {
                'min_date': ob_range.min_date,
                'max_date': ob_range.max_date,
                'total_days': (ob_range.max_date - ob_range.min_date).days,
                'days_with_data': ob_range.days_with_data,
                'total_records': ob_range.total_records,
                'valid_records': ob_range.valid_records
            },
            'gaps': gaps,
            'long_gaps': long_gaps,
            'no_data_days': len(no_data_days),
            'sparse_days': len(sparse_days),
            'daily_breakdown': daily_data,
            'june_specific': june_data,
            'problem_days': problem_days,
            'markprices_status': {
                'min_date': mp_check.min_date if mp_check else None,
                'max_date': mp_check.max_date if mp_check else None,
                'total_records': mp_check.total_records if mp_check else 0,
                'days_covered': mp_check.days_with_markprices if mp_check else 0
            }
        }

def generate_gap_report(diagnosis: Dict):
    """Generar reporte detallado de gaps"""
    log.info(f"\n{'='*80}")
    log.info(f"üìä DIAGN√ìSTICO DE GAPS - {diagnosis['symbol']}")
    log.info(f"{'='*80}")
    
    # Resumen general
    ob_info = diagnosis['orderbook_range']
    log.info(f"\nüìÖ RANGO DE DATOS ORDERBOOK:")
    log.info(f"  Per√≠odo: {ob_info['min_date'].strftime('%Y-%m-%d')} a {ob_info['max_date'].strftime('%Y-%m-%d')}")
    log.info(f"  D√≠as totales: {ob_info['total_days']}")
    log.info(f"  D√≠as con datos: {ob_info['days_with_data']} ({ob_info['days_with_data']/max(1,ob_info['total_days'])*100:.1f}%)")
    log.info(f"  Total registros: {ob_info['total_records']:,}")
    log.info(f"  Registros v√°lidos: {ob_info['valid_records']:,} ({ob_info['valid_records']/max(1,ob_info['total_records'])*100:.1f}%)")
    
    # Gaps encontrados
    if diagnosis['gaps']:
        log.warning(f"\n‚ö†Ô∏è GAPS ENCONTRADOS: {len(diagnosis['gaps'])}")
        for gap in diagnosis['gaps']:
            log.warning(f"  ‚Ä¢ {gap['start']} a {gap['end']} ({gap['days']} d√≠as) - Tipo: {gap['type']}")
        
        # Gaps largos (m√°s problem√°ticos)
        if diagnosis['long_gaps']:
            log.error(f"\nüö® GAPS LARGOS (‚â•3 d√≠as):")
            for gap in diagnosis['long_gaps']:
                log.error(f"  ‚Ä¢ {gap['start']} a {gap['end']} ({gap['days']} d√≠as)")
    else:
        log.info(f"\n‚úÖ No se encontraron gaps significativos")
    
    # D√≠as problem√°ticos
    problem_days = diagnosis.get('problem_days')
    if problem_days is not None and not problem_days.empty:
        log.warning(f"\n‚ö†Ô∏è D√çAS CON DATOS ESCASOS (<500 registros):")
        for _, day in problem_days.iterrows():
            log.warning(f"  ‚Ä¢ {day['date']}: {int(day['total_records'])} registros "
                       f"({int(day['hours_with_data'])} horas con datos, "
                       f"{day['first_time']} - {day['last_time']})")
    
    # An√°lisis de junio
    june_data = diagnosis['june_specific']
    if not june_data.empty:
        log.info(f"\nüîç AN√ÅLISIS ESPEC√çFICO DE JUNIO 2024:")
        
        # Generar rango completo de junio
        june_dates = pd.date_range('2024-06-01', '2024-06-30', freq='D')
        june_dates_df = pd.DataFrame({'date': june_dates.date})
        
        # Merge con datos existentes
        june_complete = june_dates_df.merge(
            june_data,
            on='date',
            how='left'
        )
        june_complete['orderbook_records'] = june_complete['orderbook_records'].fillna(0)
        
        june_missing = june_complete[june_complete['orderbook_records'] == 0]
        if not june_missing.empty:
            log.warning(f"  D√≠as sin datos en junio: {len(june_missing)}")
            for date in june_missing['date']:
                log.warning(f"    - {date}")
        
        june_sparse = june_complete[(june_complete['orderbook_records'] > 0) & 
                                   (june_complete['orderbook_records'] < 100)]
        if not june_sparse.empty:
            log.warning(f"  D√≠as con datos escasos en junio: {len(june_sparse)}")
            for _, row in june_sparse.iterrows():
                log.warning(f"    - {row['date']}: {int(row['orderbook_records'])} registros")
    
    # Estado de mark prices
    mp_info = diagnosis['markprices_status']
    log.info(f"\nüíé ESTADO DE MARK PRICES:")
    if mp_info['total_records'] > 0:
        log.info(f"  Per√≠odo: {mp_info['min_date'].strftime('%Y-%m-%d %H:%M')} a "
                f"{mp_info['max_date'].strftime('%Y-%m-%d %H:%M')}")
        log.info(f"  Total registros: {mp_info['total_records']:,}")
        log.info(f"  D√≠as cubiertos: {mp_info['days_covered']}")
    else:
        log.warning(f"  No hay mark prices calculados a√∫n")
    
    # Recomendaciones
    log.info(f"\nüí° RECOMENDACIONES:")
    if diagnosis['no_data_days'] > 0:
        log.info(f"  ‚ö†Ô∏è Hay {diagnosis['no_data_days']} d√≠as sin datos de orderbook")
        log.info(f"     ‚Üí Los mark prices no se pueden calcular para estos d√≠as")
        log.info(f"     ‚Üí Matplotlib conectar√° los puntos creando l√≠neas rectas")
        log.info(f"     ‚Üí Considera re-ingestar datos para esos per√≠odos con:")
        log.info(f"       python scripts/ingest_data.py --symbol {diagnosis['symbol']}")
    
    if diagnosis['sparse_days'] > 0:
        log.info(f"  ‚ö†Ô∏è Hay {diagnosis['sparse_days']} d√≠as con datos escasos")
        log.info(f"     ‚Üí Los mark prices pueden ser menos confiables estos d√≠as")
        log.info(f"     ‚Üí Verifica la fuente de datos para estos per√≠odos")
    
    if diagnosis['long_gaps']:
        log.error(f"  üö® Hay {len(diagnosis['long_gaps'])} gaps de 3+ d√≠as consecutivos")
        log.error(f"     ‚Üí Esto afectar√° significativamente los gr√°ficos y an√°lisis")
        log.error(f"     ‚Üí PRIORIDAD: Re-ingestar datos para estos per√≠odos")

def calculate_mark_prices_ultrafast(symbol: str, diagnose_only: bool = False) -> int:
    """Calcular mark prices con opci√≥n de solo diagn√≥stico"""
    
    if diagnose_only:
        try:
            # Solo hacer diagn√≥stico
            diagnosis = diagnose_data_gaps(symbol)
            generate_gap_report(diagnosis)
            
            # Guardar diagn√≥stico detallado
            diagnosis_dir = Path("diagnostics")
            diagnosis_dir.mkdir(exist_ok=True)
            
            # Guardar breakdown diario
            daily_df = diagnosis['daily_breakdown']
            symbol_short = symbol.split('_')[-2] if '_' in symbol else symbol
            daily_df.to_csv(diagnosis_dir / f"{symbol_short}_daily_gaps.csv", index=False)
            log.info(f"üìä Diagn√≥stico diario guardado en: diagnostics/{symbol_short}_daily_gaps.csv")
            
            # Guardar d√≠as problem√°ticos
            if not diagnosis['problem_days'].empty:
                diagnosis['problem_days'].to_csv(diagnosis_dir / f"{symbol_short}_problem_days.csv", index=False)
                log.info(f"üìä D√≠as problem√°ticos guardados en: diagnostics/{symbol_short}_problem_days.csv")
            
            return 0
        except Exception as e:
            log.error(f"Error en diagn√≥stico: {e}")
            import traceback
            log.error(traceback.format_exc())
            return 0
    
    log.info(f"Iniciando c√°lculo ULTRA R√ÅPIDO para {symbol}...")
    
    # Primero hacer diagn√≥stico
    try:
        diagnosis = diagnose_data_gaps(symbol)
        generate_gap_report(diagnosis)
    except Exception as e:
        log.warning(f"No se pudo completar el diagn√≥stico: {e}")
    
    with db_manager.get_session() as session:
        # Continuar con el c√°lculo normal...
        stats_query = text("""
            SELECT 
                COUNT(*) as valid
            FROM orderbook 
            WHERE symbol = :symbol
            AND bid1_price IS NOT NULL 
            AND ask1_price IS NOT NULL
            AND valid_for_trading = TRUE
            AND bid1_price > 0 
            AND ask1_price > 0
            AND bid1_price < ask1_price
        """)
        
        stats = session.execute(stats_query, {'symbol': symbol}).fetchone()
        log.info(f"Registros a procesar: {stats.valid:,}")
        
        if stats.valid == 0:
            return 0
        
        # CALCULAR TODO EN UNA SOLA QUERY SQL - ULTRA R√ÅPIDO
        log.info("Ejecutando c√°lculo masivo en SQL...")
        
        insert_query = text("""
            INSERT INTO mark_prices (
                symbol, timestamp, mark_price, orderbook_mid, 
                ohlcv_close, ohlcv_volume, liquidity_score, valid_for_trading
            )
            SELECT 
                :symbol as symbol,
                timestamp,
                
                -- Mark price = promedio de VWAP bid y ask
                (
                    -- VWAP BID (usando niveles disponibles)
                    CASE WHEN 
                        COALESCE(bid1_size, 0) + COALESCE(bid2_size, 0) + COALESCE(bid3_size, 0) + 
                        COALESCE(bid4_size, 0) + COALESCE(bid5_size, 0) > 0
                    THEN
                        (COALESCE(bid1_price * bid1_size, 0) + COALESCE(bid2_price * bid2_size, 0) + 
                         COALESCE(bid3_price * bid3_size, 0) + COALESCE(bid4_price * bid4_size, 0) + 
                         COALESCE(bid5_price * bid5_size, 0)) /
                        (COALESCE(bid1_size, 0) + COALESCE(bid2_size, 0) + COALESCE(bid3_size, 0) + 
                         COALESCE(bid4_size, 0) + COALESCE(bid5_size, 0))
                    ELSE bid1_price
                    END
                    +
                    -- VWAP ASK (usando niveles disponibles)
                    CASE WHEN 
                        COALESCE(ask1_size, 0) + COALESCE(ask2_size, 0) + COALESCE(ask3_size, 0) + 
                        COALESCE(ask4_size, 0) + COALESCE(ask5_size, 0) > 0
                    THEN
                        (COALESCE(ask1_price * ask1_size, 0) + COALESCE(ask2_price * ask2_size, 0) + 
                         COALESCE(ask3_price * ask3_size, 0) + COALESCE(ask4_price * ask4_size, 0) + 
                         COALESCE(ask5_price * ask5_size, 0)) /
                        (COALESCE(ask1_size, 0) + COALESCE(ask2_size, 0) + COALESCE(ask3_size, 0) + 
                         COALESCE(ask4_size, 0) + COALESCE(ask5_size, 0))
                    ELSE ask1_price
                    END
                ) / 2.0 as mark_price,
                
                -- Orderbook mid simple
                (bid1_price + ask1_price) / 2.0 as orderbook_mid,
                
                -- OHLCV inicialmente NULL
                NULL as ohlcv_close,
                NULL as ohlcv_volume,
                
                -- Liquidity score basado en niveles disponibles
                LEAST(1.0, 
                    (CASE WHEN bid1_size IS NOT NULL AND bid1_size > 0 THEN 1 ELSE 0 END +
                     CASE WHEN bid2_size IS NOT NULL AND bid2_size > 0 THEN 1 ELSE 0 END +
                     CASE WHEN bid3_size IS NOT NULL AND bid3_size > 0 THEN 1 ELSE 0 END +
                     CASE WHEN bid4_size IS NOT NULL AND bid4_size > 0 THEN 1 ELSE 0 END +
                     CASE WHEN bid5_size IS NOT NULL AND bid5_size > 0 THEN 1 ELSE 0 END +
                     CASE WHEN ask1_size IS NOT NULL AND ask1_size > 0 THEN 1 ELSE 0 END +
                     CASE WHEN ask2_size IS NOT NULL AND ask2_size > 0 THEN 1 ELSE 0 END +
                     CASE WHEN ask3_size IS NOT NULL AND ask3_size > 0 THEN 1 ELSE 0 END +
                     CASE WHEN ask4_size IS NOT NULL AND ask4_size > 0 THEN 1 ELSE 0 END +
                     CASE WHEN ask5_size IS NOT NULL AND ask5_size > 0 THEN 1 ELSE 0 END) / 8.0
                ) as liquidity_score,
                
                TRUE as valid_for_trading
                
            FROM orderbook 
            WHERE symbol = :symbol
            AND bid1_price IS NOT NULL 
            AND ask1_price IS NOT NULL
            AND valid_for_trading = TRUE
            AND bid1_price > 0 
            AND ask1_price > 0
            AND bid1_price < ask1_price
            AND bid1_size > 0
            AND ask1_size > 0
            
            ON CONFLICT (symbol, timestamp) DO NOTHING
        """)
        
        start_time = datetime.now()
        result = session.execute(insert_query, {'symbol': symbol})
        end_time = datetime.now()
        
        duration = (end_time - start_time).total_seconds()
        log.info(f"C√°lculo SQL completado en {duration:.1f} segundos")
        log.info(f"Registros procesados: {result.rowcount:,}")
        
        return result.rowcount

def update_ohlcv_ultrafast(symbol: str):
    """Actualizaci√≥n ultra r√°pida de OHLCV en una sola query"""
    with db_manager.get_session() as session:
        start_time = datetime.now()
        
        update_query = text("""
            UPDATE mark_prices 
            SET 
                ohlcv_close = subq.close,
                ohlcv_volume = subq.volume
            FROM (
                SELECT 
                    timestamp,
                    close,
                    volume
                FROM ohlcv 
                WHERE symbol = :symbol
            ) subq
            WHERE mark_prices.symbol = :symbol
            AND mark_prices.timestamp = subq.timestamp
            AND (mark_prices.ohlcv_close IS NULL OR mark_prices.ohlcv_volume IS NULL)
        """)
        
        result = session.execute(update_query, {'symbol': symbol})
        end_time = datetime.now()
        
        duration = (end_time - start_time).total_seconds()
        log.info(f"OHLCV actualizado en {duration:.1f}s - {result.rowcount:,} registros")

def get_progress_estimate(symbol: str) -> Dict:
    """Estimar progreso si hay datos existentes"""
    with db_manager.get_session() as session:
        # Total registros en orderbook v√°lidos
        total_query = text("""
            SELECT COUNT(*) as total
            FROM orderbook 
            WHERE symbol = :symbol
            AND bid1_price IS NOT NULL 
            AND ask1_price IS NOT NULL
            AND valid_for_trading = TRUE
            AND bid1_price > 0 
            AND ask1_price > 0
            AND bid1_price < ask1_price
            AND bid1_size > 0
            AND ask1_size > 0
        """)
        total = session.execute(total_query, {'symbol': symbol}).fetchone().total
        
        # Registros ya procesados en mark_prices
        processed_query = text("""
            SELECT COUNT(*) as processed
            FROM mark_prices 
            WHERE symbol = :symbol
        """)
        processed = session.execute(processed_query, {'symbol': symbol}).fetchone().processed
        
        return {
            'total': total,
            'processed': processed,
            'remaining': total - processed,
            'progress_pct': (processed / max(1, total)) * 100
        }

def main():
    """Funci√≥n principal con diagn√≥stico mejorado"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Calculate mark prices with gap diagnostics")
    parser.add_argument("--symbol", type=str, help="Specific symbol to process")
    parser.add_argument("--force", action="store_true", help="Force recalculation")
    parser.add_argument("--diagnose-only", action="store_true", help="Only run diagnostics, don't calculate")
    
    args = parser.parse_args()
    
    if args.diagnose_only:
        log.info("üîç Ejecutando solo diagn√≥stico de gaps...")
    else:
        log.info("Iniciando c√°lculo de mark prices con diagn√≥stico...")
    
    # Obtener s√≠mbolos
    if args.symbol:
        symbols = [args.symbol]
    else:
        try:
            active_pairs = settings.get_active_pairs()
            symbols = []
            for pair in active_pairs:
                symbols.extend([pair.symbol1, pair.symbol2])
            symbols = list(set(symbols))
        except Exception as e:
            log.error(f"Error cargando s√≠mbolos: {e}")
            symbols = ['MEXCFTS_PERP_GIGA_USDT', 'MEXCFTS_PERP_SPX_USDT']
    
    if not symbols:
        log.error("No hay s√≠mbolos para procesar")
        return False
    
    log.info(f"Procesando {len(symbols)} s√≠mbolos")
    
    try:
        total_processed = 0
        
        for symbol in symbols:
            log.info(f"\n{'='*60}")
            log.info(f"PROCESANDO {symbol}")
            log.info(f"{'='*60}")
            
            if args.diagnose_only:
                # Solo diagn√≥stico
                calculate_mark_prices_ultrafast(symbol, diagnose_only=True)
                continue
            
            # Verificar progreso
            if not args.force:
                progress = get_progress_estimate(symbol)
                if progress['processed'] > 0:
                    log.info(f"Progreso existente: {progress['processed']:,}/{progress['total']:,} ({progress['progress_pct']:.1f}%)")
                    
                    if progress['progress_pct'] >= 99:
                        log.info(f"‚úÖ {symbol} ya est√° 99%+ completo - saltando procesamiento")
                        continue
            
            if args.force:
                try:
                    with db_manager.get_session() as session:
                        result = session.execute(text("DELETE FROM mark_prices WHERE symbol = :symbol"), 
                                              {'symbol': symbol})
                        log.info(f"üóëÔ∏è Eliminados {result.rowcount} mark prices existentes")
                except Exception as e:
                    log.warning(f"Error eliminando datos existentes: {e}")
            
            start_time = datetime.now()
            processed_count = calculate_mark_prices_ultrafast(symbol)
            
            if processed_count > 0:
                update_ohlcv_ultrafast(symbol)
                total_processed += processed_count
                
                end_time = datetime.now()
                duration = (end_time - start_time).total_seconds()
                rate = processed_count / duration if duration > 0 else 0
                
                log.info(f"‚úÖ {processed_count:,} mark prices calculados para {symbol}")
                log.info(f"   Tiempo total: {duration:.1f}s ({rate:.0f} records/sec)")
            else:
                log.info(f"‚ÑπÔ∏è No se agregaron nuevos mark prices para {symbol}")
        
        if not args.diagnose_only:
            log.info(f"\nüéâ C√°lculo completado!")
            log.info(f"Total procesados: {total_processed:,} mark prices")
        else:
            log.info(f"\nüîç Diagn√≥stico completado!")
            log.info(f"Revisa los archivos en diagnostics/ para m√°s detalles")
            log.info(f"\nüí° Si hay gaps significativos:")
            log.info(f"  1. Los mark prices no se calcular√°n para esos per√≠odos")
            log.info(f"  2. Los gr√°ficos mostrar√°n l√≠neas rectas (interpolaci√≥n)")
            log.info(f"  3. Considera re-ingestar datos para esos per√≠odos")
        
        return True
        
    except Exception as e:
        log.error(f"Error: {e}")
        import traceback
        log.error(traceback.format_exc())
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)